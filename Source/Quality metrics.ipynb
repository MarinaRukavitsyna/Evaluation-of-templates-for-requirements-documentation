{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import re\n",
    "import readability # https://pypi.org/project/readability/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSheets = pd.read_excel('../Data/TemplateComparisonAnalytics.xlsx', \n",
    "                         header = 4,\n",
    "                         sheet_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>name_of_template</th>\n",
       "      <th>liability? (IR1,E106,Rupp09)</th>\n",
       "      <th>structured_sentence?  (IR2,SR14-15,E106)</th>\n",
       "      <th>#sentences (IR11+18,SR9,E106)</th>\n",
       "      <th>#words (SR15)</th>\n",
       "      <th>#words/sentence</th>\n",
       "      <th>#characters</th>\n",
       "      <th>#syllables</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 64</th>\n",
       "      <th>Unnamed: 65</th>\n",
       "      <th>Unnamed: 66</th>\n",
       "      <th>Unnamed: 67</th>\n",
       "      <th>Unnamed: 68</th>\n",
       "      <th>Unnamed: 69</th>\n",
       "      <th>Unnamed: 70</th>\n",
       "      <th>Unnamed: 71</th>\n",
       "      <th>Unnamed: 72</th>\n",
       "      <th>Unnamed: 73</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>The Thermal Control shall fulfil the requireme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>115</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>The list of requirements from [ND-E-15] and [N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>141</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>The Thermal Control shall provide the thermal ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>159</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>The Thermal Control shall ensure temperatures ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>137</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The Thermal Control shall ensure the temperatu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>110</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 74 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                               Text  name_of_template  \\\n",
       "0  1.0  The Thermal Control shall fulfil the requireme...               NaN   \n",
       "1  2.0  The list of requirements from [ND-E-15] and [N...               NaN   \n",
       "2  3.0  The Thermal Control shall provide the thermal ...               NaN   \n",
       "3  4.0  The Thermal Control shall ensure temperatures ...               NaN   \n",
       "4  5.0  The Thermal Control shall ensure the temperatu...               NaN   \n",
       "\n",
       "   liability? (IR1,E106,Rupp09)  structured_sentence?  (IR2,SR14-15,E106)  \\\n",
       "0                           1.0                                       1.0   \n",
       "1                           0.0                                       0.0   \n",
       "2                           1.0                                       1.0   \n",
       "3                           1.0                                       1.0   \n",
       "4                           1.0                                       1.0   \n",
       "\n",
       "   #sentences (IR11+18,SR9,E106)  #words (SR15)  #words/sentence  #characters  \\\n",
       "0                              1             18               18          115   \n",
       "1                              1             26               26          141   \n",
       "2                              1             29               29          159   \n",
       "3                              1             22               22          137   \n",
       "4                              1             16               16          110   \n",
       "\n",
       "   #syllables  ...  Unnamed: 64  Unnamed: 65  Unnamed: 66  Unnamed: 67  \\\n",
       "0        32.0  ...          NaN          NaN          NaN          NaN   \n",
       "1        45.0  ...          NaN          NaN          NaN          NaN   \n",
       "2        51.0  ...          NaN          NaN          NaN          NaN   \n",
       "3        44.0  ...          NaN          NaN          NaN          NaN   \n",
       "4        36.0  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 68  Unnamed: 69  Unnamed: 70  Unnamed: 71  Unnamed: 72  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 73  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 74 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSheets[\"FLEX free\"].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_FScore(text):\n",
    "    # function to test if something is a noun\n",
    "    is_noun = lambda pos: pos[:2] == 'NN'\n",
    "    # function to test if something is a pronoun\n",
    "    is_pronoun = lambda pos: pos[:3] == 'PRP'\n",
    "    # function to test if something is an adjective\n",
    "    is_adj = lambda pos: pos[:2] == 'JJ'\n",
    "    # function to test if something is a preposition\n",
    "    is_prep = lambda pos: pos[:2] == 'IN'\n",
    "    # function to test if something is an article\n",
    "    is_article = lambda pos: pos[:2] == 'DT'\n",
    "    # function to test if something is a verb\n",
    "    is_verb = lambda pos: pos[:2] == 'VB'\n",
    "    # function to test if something is an adverb\n",
    "    is_adverb = lambda pos: pos[:2] == 'RB'\n",
    "    # function to test if something is a interjection\n",
    "    is_interj = lambda pos: pos[:2] == 'UH'\n",
    "\n",
    "    # do the nlp stuff\n",
    "    tokenized = nltk.word_tokenize(text)\n",
    "    nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
    "    pronouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_pronoun(pos)] \n",
    "    adjectives = [word for (word, pos) in nltk.pos_tag(tokenized) if is_adj(pos)] \n",
    "    prepositions = [word for (word, pos) in nltk.pos_tag(tokenized) if is_prep(pos)] \n",
    "    articles = [word for (word, pos) in nltk.pos_tag(tokenized) if is_article(pos)] \n",
    "    verbs = [word for (word, pos) in nltk.pos_tag(tokenized) if is_verb(pos)] \n",
    "    adverbs = [word for (word, pos) in nltk.pos_tag(tokenized) if is_adverb(pos)] \n",
    "    interjections = [word for (word, pos) in nltk.pos_tag(tokenized) if is_interj(pos)] \n",
    "    \n",
    "    textLen = len(text)\n",
    "\n",
    "    FScore = (len(nouns)/textLen*100 + len(adjectives)/textLen*100 + len(prepositions)/textLen*100 + len(articles)/textLen*100 - len(pronouns)/textLen*100 - len(verbs)/textLen*100 - len(adverbs)/textLen*100 - len(interjections)/textLen*100 + 100)/2\n",
    "      \n",
    "    return FScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word) :\n",
    "    word = word.lower()\n",
    "\n",
    "    # exception_add are words that need extra syllables\n",
    "    # exception_del are words that need less syllables\n",
    "\n",
    "    exception_add = ['serious','crucial']\n",
    "    exception_del = ['fortunately','unfortunately']\n",
    "\n",
    "    co_one = ['cool','coach','coat','coal','count','coin','coarse','coup','coif','cook','coign','coiffe','coof','court']\n",
    "    co_two = ['coapt','coed','coinci']\n",
    "\n",
    "    pre_one = ['preach']\n",
    "\n",
    "    syls = 0 #added syllable number\n",
    "    disc = 0 #discarded syllable number\n",
    "\n",
    "    #1) if letters < 3 : return 1\n",
    "    if len(word) <= 3 :\n",
    "        syls = 1\n",
    "        return syls\n",
    "\n",
    "    #2) if doesn't end with \"ted\" or \"tes\" or \"ses\" or \"ied\" or \"ies\", discard \"es\" and \"ed\" at the end.\n",
    "    # if it has only 1 vowel or 1 set of consecutive vowels, discard. (like \"speed\", \"fled\" etc.)\n",
    "\n",
    "    if word[-2:] == \"es\" or word[-2:] == \"ed\" :\n",
    "        doubleAndtripple_1 = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "        if doubleAndtripple_1 > 1 or len(re.findall(r'[eaoui][^eaoui]',word)) > 1 :\n",
    "            if word[-3:] == \"ted\" or word[-3:] == \"tes\" or word[-3:] == \"ses\" or word[-3:] == \"ied\" or word[-3:] == \"ies\" :\n",
    "                pass\n",
    "            else :\n",
    "                disc+=1\n",
    "\n",
    "    #3) discard trailing \"e\", except where ending is \"le\"  \n",
    "\n",
    "    le_except = ['whole','mobile','pole','male','female','hale','pale','tale','sale','aisle','whale','while']\n",
    "\n",
    "    if word[-1:] == \"e\" :\n",
    "        if word[-2:] == \"le\" and word not in le_except :\n",
    "            pass\n",
    "\n",
    "        else :\n",
    "            disc+=1\n",
    "\n",
    "    #4) check if consecutive vowels exists, triplets or pairs, count them as one.\n",
    "\n",
    "    doubleAndtripple = len(re.findall(r'[eaoui][eaoui]',word))\n",
    "    tripple = len(re.findall(r'[eaoui][eaoui][eaoui]',word))\n",
    "    disc+=doubleAndtripple + tripple\n",
    "\n",
    "    #5) count remaining vowels in word.\n",
    "    numVowels = len(re.findall(r'[eaoui]',word))\n",
    "\n",
    "    #6) add one if starts with \"mc\"\n",
    "    if word[:2] == \"mc\" :\n",
    "        syls+=1\n",
    "\n",
    "    #7) add one if ends with \"y\" but is not surrouned by vowel\n",
    "    if word[-1:] == \"y\" and word[-2] not in \"aeoui\" :\n",
    "        syls +=1\n",
    "\n",
    "    #8) add one if \"y\" is surrounded by non-vowels and is not in the last word.\n",
    "\n",
    "    for i,j in enumerate(word) :\n",
    "        if j == \"y\" :\n",
    "            if (i != 0) and (i != len(word)-1) :\n",
    "                if word[i-1] not in \"aeoui\" and word[i+1] not in \"aeoui\" :\n",
    "                    syls+=1\n",
    "\n",
    "    #9) if starts with \"tri-\" or \"bi-\" and is followed by a vowel, add one.\n",
    "\n",
    "    if word[:3] == \"tri\" and word[3] in \"aeoui\" :\n",
    "        syls+=1\n",
    "\n",
    "    if word[:2] == \"bi\" and word[2] in \"aeoui\" :\n",
    "        syls+=1\n",
    "\n",
    "    #10) if ends with \"-ian\", should be counted as two syllables, except for \"-tian\" and \"-cian\"\n",
    "\n",
    "    if word[-3:] == \"ian\" : \n",
    "    #and (word[-4:] != \"cian\" or word[-4:] != \"tian\") :\n",
    "        if word[-4:] == \"cian\" or word[-4:] == \"tian\" :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #11) if starts with \"co-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "    if word[:2] == \"co\" and word[2] in 'eaoui' :\n",
    "\n",
    "        if word[:4] in co_two or word[:5] in co_two or word[:6] in co_two :\n",
    "            syls+=1\n",
    "        elif word[:4] in co_one or word[:5] in co_one or word[:6] in co_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #12) if starts with \"pre-\" and is followed by a vowel, check if exists in the double syllable dictionary, if not, check if in single dictionary and act accordingly.\n",
    "\n",
    "    if word[:3] == \"pre\" and word[3] in 'eaoui' :\n",
    "        if word[:6] in pre_one :\n",
    "            pass\n",
    "        else :\n",
    "            syls+=1\n",
    "\n",
    "    #13) check for \"-n't\" and cross match with dictionary to add syllable.\n",
    "\n",
    "    negative = [\"doesn't\", \"isn't\", \"shouldn't\", \"couldn't\",\"wouldn't\"]\n",
    "\n",
    "    if word[-3:] == \"n't\" :\n",
    "        if word in negative :\n",
    "            syls+=1\n",
    "        else :\n",
    "            pass   \n",
    "\n",
    "    #14) Handling the exceptional words.\n",
    "\n",
    "    if word in exception_del :\n",
    "        disc+=1\n",
    "\n",
    "    if word in exception_add :\n",
    "        syls+=1     \n",
    "\n",
    "    # calculate the output\n",
    "    return numVowels - disc + syls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add definite_articles?  (IR5,SR10-11)\n",
    "def if_definite_articles(text): \n",
    "    article_terms = ['a']  \n",
    "    \n",
    "    doc = nlp(text.lower()) \n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in article_terms:\n",
    "            return 0  \n",
    "     \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_nominalization? (SR3)\n",
    "def no_nominalization(text):    \n",
    "    result = readability.getmeasures(text, lang='en')['word usage']['nominalization']\n",
    "    return (1 if result == 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_comparison? (SR8,Rupp09)\n",
    "# Add clear_comparison? (SR8) \n",
    "\n",
    "def no_comparison(text):\n",
    "    doc = nlp(text) \n",
    "    for token in doc:\n",
    "        if (token.tag_ == \"RBR\" or token.tag_ == \"JJR\"): \n",
    "            return 0        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add units? (IR6)\n",
    "def correct_units(text):\n",
    "    for i in text:\n",
    "        if i.isdigit():\n",
    "            return 0\n",
    "        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_vague_terms? (IR7,SR2+12,E106)\n",
    "def no_vague_terms(text):\n",
    "    vague_terms_simple = ['some', 'any', 'allowable ', 'several', 'many', 'nearly ', 'about', 'almost',\n",
    "                          'approximate', 'ancillary','relevant', 'routine', 'common', 'generic', 'significant', \n",
    "                          'flexible', 'expandable', 'typical', 'sufficient', 'adequate', 'appropriate', 'efficient', \n",
    "                          'effective', 'proficient', 'reasonable', 'customary', 'usually', 'approximately', \n",
    "                          'sufficiently', 'typically']  \n",
    "    vague_terms_complex = ['a lot of', 'a few', 'almost always', 'very nearly',  'close to'] \n",
    "    \n",
    "    doc = nlp(text.lower())     \n",
    "    for token in doc:\n",
    "        if token.text in vague_terms_simple:\n",
    "            return 0  \n",
    "     \n",
    "    result = [word for word in vague_terms_complex if(word in text.lower())] \n",
    "    \n",
    "    return (0 if bool(result) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_escape_clause? (IR8)\n",
    "def no_escape_clause(text):\n",
    "    escape_clause = ['so far as is possible', 'as little as possible', 'where possible', \n",
    "                     'as much as possible', 'if it should prove necessary', 'if necessary', \n",
    "                     'to the extent necessary', 'as appropriate', 'as required', \n",
    "                     'to the extent practical', 'if practicable']  \n",
    "    \n",
    "    result = [word for word in escape_clause if(word in text.lower())] \n",
    "    \n",
    "    return (0 if bool(result) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_open_end? (IR9)\n",
    "def no_open_end(text):    \n",
    "    open_end_terms_simple = ['etc']  \n",
    "    open_end_terms_complex = ['and so on', 'including but not limited to']  \n",
    "    \n",
    "    doc = nlp(text.lower())     \n",
    "    for token in doc:\n",
    "        if token.text in open_end_terms_simple:\n",
    "            return 0  \n",
    "     \n",
    "    result = [word for word in open_end_terms_complex if(word in text.lower())] \n",
    "    \n",
    "    return (0 if bool(result) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_superfluous_infinitives? (IR10)\n",
    "def no_superfluous_infinitives(text):\n",
    "    superfluous_infinitives = ['be designed', 'be able to', 'be capable to', 'be capable of']  \n",
    "    \n",
    "    result = [word for word in superfluous_infinitives if(word in text.lower())] \n",
    "    \n",
    "    return (0 if bool(result) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_negation? (IR16,E106)\n",
    "def no_negation(text):\n",
    "    negation_terms = ['not']  \n",
    "    \n",
    "    doc = nlp(text.lower()) \n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in negation_terms:\n",
    "            return 0  \n",
    "     \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_combinators? (IR19,SR9,E106)\n",
    "def no_combinators(text):\n",
    "    combinator_terms_simple = ['and', 'or', 'then', 'unless', 'but', 'however', 'also','whether', 'meanwhile',\n",
    "                               'whereas', 'otherwise']  \n",
    "    combinator_terms_complex = ['as well as', 'but also', 'on the other hand', ]  \n",
    "    \n",
    "    doc = nlp(text.lower())     \n",
    "    for token in doc:\n",
    "        if token.text in combinator_terms_simple:\n",
    "            return 0  \n",
    "     \n",
    "    result = [word for word in combinator_terms_complex if(word in text.lower())] \n",
    "    \n",
    "    return (0 if bool(result) else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clear_quantifiers? (IR32+34,SR8+10-11,E106)\n",
    "def clear_quantifiers(text):\n",
    "    quantifiers_terms = ['all', 'any', 'both”']  \n",
    "    \n",
    "    doc = nlp(text.lower()) \n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in quantifiers_terms:\n",
    "            return 0  \n",
    "     \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_absolutes? (IR26) - evaluate cells with '0' manually\n",
    "def no_absolutes(text):\n",
    "    absolutes_terms = ['100%', '100 %', 'all', 'always', 'never']  \n",
    "    \n",
    "    doc = nlp(text.lower()) \n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in absolutes_terms:\n",
    "            return 0  \n",
    "     \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add no_pronouns? (IR24)\n",
    "def no_pronouns(text):\n",
    "    doc = nlp(text.lower()) \n",
    "    for token in doc:\n",
    "        if (token.pos_ == \"PRON\"):\n",
    "            return 0        \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel sheet column names used for calculation\n",
    "textHeader = \"Text\"\n",
    "FScoreHeader = \"FScore\"\n",
    "syllablesHeader = \"#syllables\"\n",
    "definite_articlesHeader = \"definite_articles?  (IR5,SR10-11)\"\n",
    "no_nominalizationHeader = \"no_nominalization? (SR3)\"\n",
    "no_comparisonHeader = \"no_comparison? (SR8,Rupp09)\"\n",
    "clear_comparisonHeader = \"clear_comparison? (SR8)\"\n",
    "unitsHeader = \"units? (IR6)\"\n",
    "no_vague_termsHeader = \"no_vague_terms? (IR7,SR2+12,E106)\"\n",
    "no_escape_clauseHeader = \"no_escape_clause? (IR8)\"\n",
    "no_open_endHeader = \"no_open_end? (IR9)\"\n",
    "no_superfluous_infinitivesHeader = \"no_superfluous_infinitives? (IR10)\"\n",
    "no_negationHeader = \"no_negation? (IR16,E106)\"\n",
    "no_combinatorsHeader = \"no_combinators? (IR19,SR9,E106)\"\n",
    "no_pronounsHeader = \"no_pronouns? (IR24)\"\n",
    "no_absolutesHeader = \"no_absolutes? (IR26)\"\n",
    "clear_quantifiersHeader = \"clear_quantifiers? (IR32+34,SR8+10-11,E106)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLEX free: processed\n",
      "FLEX EARS: processed\n",
      "FLEX MASTER: processed\n",
      "FLEX advEARS: processed\n",
      "FLEX boilerplates: processed\n",
      "FLEX boilerplates (DODT): processed\n",
      "FLEX SPIDER: processed\n",
      "ECSS_E60-30 free: processed\n",
      "ECSS_E60-30 EARS: processed\n",
      "ECSS_E60-30 MASTER: processed\n",
      "ECSS_E60-30 advEARS: processed\n",
      "ECSS_E60-30 boilerplates: processed\n",
      "ECSS_E60-30 boilerplates (DODT): processed\n",
      "ECSS_E60-30 SPIDER: processed\n",
      "CS_E 50 free: processed\n",
      "CS_E 50 EARS: processed\n",
      "CS_E 50 MASTER: processed\n",
      "CS_E 50 advEARS: processed\n",
      "CS_E 50 boilerplates: processed\n",
      "CS_E 50 boilerplates (DODT): processed\n",
      "CS_E 50 SPIDER: processed\n",
      "TSS free: processed\n",
      "TSS EARS: processed\n",
      "TSS MASTER: processed\n",
      "TSS advEARS: processed\n",
      "TSS boilerplates: processed\n",
      "TSS boilerplates (DODT): processed\n",
      "TSS SPIDER: processed\n",
      "EVS free: processed\n",
      "EVS EARS: processed\n",
      "EVS MASTER: processed\n",
      "EVS advEARS: processed\n",
      "EVS boilerplates: processed\n",
      "EVS boilerplates (DODT): processed\n",
      "EVS SPIDER: processed\n"
     ]
    }
   ],
   "source": [
    "for dfName, df in dfSheets.items():\n",
    "    if dfName == \"Summary\":\n",
    "        break            \n",
    "        \n",
    "    filtered_df = df[df[textHeader].notnull()]\n",
    "    df[syllablesHeader] = filtered_df[textHeader].apply(count_syllables)\n",
    "    df[definite_articlesHeader] = filtered_df[textHeader].apply(if_definite_articles)\n",
    "    df[no_nominalizationHeader] = filtered_df[textHeader].apply(no_nominalization)\n",
    "    df[no_comparisonHeader] = df[clear_comparisonHeader] = filtered_df[textHeader].apply(no_comparison)\n",
    "    df[unitsHeader] = filtered_df[textHeader].apply(correct_units) \n",
    "    df[no_vague_termsHeader] = filtered_df[textHeader].apply(no_vague_terms)\n",
    "    df[no_escape_clauseHeader] = filtered_df[textHeader].apply(no_escape_clause)\n",
    "    df[no_open_endHeader] = filtered_df[textHeader].apply(no_open_end)\n",
    "    df[no_superfluous_infinitivesHeader] = filtered_df[textHeader].apply(no_superfluous_infinitives)\n",
    "    df[no_negationHeader] = filtered_df[textHeader].apply(no_negation)\n",
    "    df[no_combinatorsHeader] = filtered_df[textHeader].apply(no_combinators)\n",
    "        \n",
    "    df[no_pronounsHeader] = filtered_df[textHeader].apply(no_pronouns)\n",
    "    df[no_absolutesHeader] = filtered_df[textHeader].apply(no_absolutes)    \n",
    "    df[clear_quantifiersHeader] = filtered_df[textHeader].apply(clear_quantifiers)\n",
    "    \n",
    "    # Calculate F-Score\n",
    "    corpus = ' '.join(filtered_df[textHeader])\n",
    "    FScore = calc_FScore(corpus) \n",
    "    df.insert(2, FScoreHeader, np.nan, 1) \n",
    "    df.at[0, FScoreHeader] = FScore\n",
    "    \n",
    "    print(dfName + \": processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('TemplateComparison_calculatedMetrics.xlsx', engine='xlsxwriter')\n",
    "\n",
    "for dfName, df in dfSheets.items():\n",
    "    if dfName == \"Summary\":\n",
    "        break\n",
    "        \n",
    "    df.to_excel(writer, dfName, \n",
    "                columns = [textHeader, syllablesHeader, definite_articlesHeader, no_nominalizationHeader, \n",
    "                           no_comparisonHeader, unitsHeader, no_vague_termsHeader, no_escape_clauseHeader, \n",
    "                           no_superfluous_infinitivesHeader, no_negationHeader, no_combinatorsHeader, \n",
    "                           no_pronounsHeader, no_absolutesHeader, clear_quantifiersHeader, FScoreHeader])\n",
    "\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
